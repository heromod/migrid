#summary Developer notes by Jost and Benjamin about integrating zero-install with MiG

= NOTES to integrate zero-install as RE provider =

== Goal ==
  * select REs which are provided by a zero-install (ZI) machinery
  * Data from the swrepo is digested to a presentation of available REs in .job submission page and .RE page

When submitting a job:
  * detect REs which are provided by ZI 
  * if any such env. has been specified, add ZI to RE requirements
  * replace the RE by suitable definitions of env. variables (see below for details)
  * then, submit job as usual

== Work Items ==
  * modify/extend Zero-install XML format 
  * RE database page
  * Job submission page
  * Job submission handler + modifications for actual execution (We are here)
  * Preparations: Software repository on portal, MiG resources, ARC resources

=== Zero-install XML format ===

    XML should indicate executable binaries in a zero-install package

A ZI feed does not yet provide information which binaries are included inside it.

For information about the ZI feed format see http://0install.net/interface-spec.html
Any extension with namespace should not harm 0install. Our extension defines the following elements:
{{{
<binaries xmlns="http://portal.grid.dk/0install/namespace">
  <binary name="bl2seq"/>
  <binary name="blast2"/>
  <binary name="/usr/bin/blastall"/>
  ...
</binaries>
}}}
as a child element of `<interface>`. 

The MiG-RE  derived from this XML will use the binaries' basenames, capitalised, as 
variable names, their values being a call to 0launch using the full name (path) as the main program (-m flag). Corresponding env.var.s will be added to the env. when submitting to the resource.

A ZI feed may provide binary names or leave this feature out, in which case the only variable which is defined is the capitalised package name, calling the default main.

QUESTION: what to do about duplicate definitions (native + 0install)
   ZI provided REs should not duplicate existing native REs. A resource can implement any RE natively, but has to correspond to the generated ZI RE in this case.

TODO: the swrepo management code should be modified to use the griddk namespace. 

The interface.xsl file already contains pieces for it, and the new binary fields will be in the other namespace anyway.

While we are at it, prettify the page using XML stylesheets. http://en.wikipedia.org/wiki/XSLT

TODO LATER: integrate in the MiG style pages (menu etc).

=== RE lib ("db") page with ZI provided entries ===

  * ZI configuration items needed in MiGserver.conf:
     * One configuration item providing the ZI RE name and variable name
     * a second config. item containing the URL (already there)
     * a third config. item containing the local path to repo.conf
  (but direct file access can eventually be replaced by http access and xml parsing)

  * A new method `list_0install_res()` is provided to pull ZI RE names from the system
  * the MiG method `list_runtime_envs` includes these ZI RE names
  * a check `is_0install_re` for RE names is provided
  * the MiG method `is_runtime_env` is extended by this information.

Once the URLs retrieved, a RE entry is generated from an XML feed (enhanced as above)
  * reading XML from the URL via HTTP (to allow remote packages, for instance official ZI packages)
  * using a minidom XML parser
  * constructing a MiG RE entry (dictionary) and checking the entry

  * the generated entries are added to the MiG RE page

The overhead for reading the repo.conf file repeatedly is not a problem. MiG could use a cache file for this, but should not generate RE files (too static).
  * (Consider using REST interface instead of repo.conf)

=== submission pages enhanced with ZI REs ===
  * using the configurable local file path (to repo.conf) as above
  * The new submission page will include information about whether or not a RE is a ZI RE. 
  * For ARC jobs, only ZI REs will be offered to the user (javascript)

==== submission handler enhanced with 0install additions ====
ZI REs are used when submitting a job to a resource (only at that point, it is clear which REs are provided natively on the executing resource).

In the back end (jobscriptgenerator.py), if a RE is provided by ZI: 
  * it is removed from the RE requirements before generating the job script
  * an env.var. is added for every binary specified in the ZI feed 

  * The scheduler needs to see/consider these _real_ RE requirements. Can be done by 
     * ~~modifying the job in the job queue (but not the file)~~. In that case, the job needs to be retouched after stopping the server (automatic re-queueing of new jobs if job queue could not be saved).

     * making resources automatically provide all ZI REs when they support ZI 
 The place to make this change is (non-obviously) the file scheduler.py. The scheduler API call to schedule a job takes a resource configuration as parameter, but uses only the resource ID and then accesses an internal cache. Instead of cross-cutting changes to create (and update) that cache, we directly and only modify the place of the RE check inside scheduler.py. 

  * The user and frontend will never see the changes to the job. The mRSL file remains untouched (for resubmission), we only "enrich" the job definition when generating the script for a particular resource (see jobscriptgenerator::create_job_script and create_arc_job)

=== SW Repo should serve feeds from portal.grid.dk ===
  * need to generate server's gpg key (store it in MiG-certificates dir)
  * need to relocate and re-sign existing ZI feeds with this server's key 
cascading to their dependencies. Needs a `zeroinstall-injector` installation on portal.grid.dk. http://0install.net/install-linux.html (use centos rpm!)
  * need to put packages on the server, accessible through plain http (aliasing a respective directory for the swrepo)

=== MiG resources: need to deploy key of our server ===
  * (can do it manually by using one of the packages once manually)
  * This is for all resources providing the ZEROINSTALL (native) RE

=== ARC resources: ===
  * need to make 0install work! (problems on benedict when run inside ARC jobs, no other cluster available for tests)
  * need to set 'XDG_CONFIG_HOME' (+friends) for ZERO_INSTALL RE
  * send a request for this RE definition (to whom?? Who helps?)
  * need to deploy our server key

-------

== OLD: Misc notes ==

=== Authentication problem with zero-install: ===

Software has to be approved by trusting the signing key for a
downloaded package.

When a new software is downloaded and used for the first time, a
message will pop up asking the user to trust the author (unless
already known with this site). Trusted keys are stored per-URL.

When submitting a job through ARC, the trust database (an XML file)
cannot be located (HOME is set to be the session directory)

Hack around: The following variables (not documented) influence where 0install searches for keystore and package cache:

'XDG_DATA_HOME',
'XDG_CACHE_HOME' ( os.path.join(home, '.cache'))

'XDG_CONFIG_HOME'

Solutions
  # Set these variables for the grid user when using ARC-RE ZERO-INSTALL
  Effect: global package cache (OK) and global keys (bad, need deployment)
  # Include server key to accept as an input file in session
  Effect: key acceptance control per-job (good), no package cache (bad)

=== Notes Benjamin, May 03 ===
{{{
- runtime environment page:
    - add info about binaries/alias(es) defined by the package
    - present information by pulling xml from .conf file
    - another table with 0-install apps
    - sort table using table sorter
    - remove sw catalog link
- job submit:
    - create new page / rewrite page
    - make detail levels or field groups
    - filter REs (when enabling arc, architecture?, ) (this will catch mismatching RE<->resource)
    - make "arc" check box
- arc
    - get an arc resource with zero install for testing
    - modify mrsl file in mrslparser.py before submitting
    -
- job script should implement the requested RE by additional environment vars
}}}